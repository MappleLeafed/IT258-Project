{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, window_size: int = 5, stride: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: DataFrame with columns [Open, Close, High, Low, return_ratio, \n",
    "                  %change_open, %change_high, %change_low, MA5, MA10, MA15, MA20, MA25, MA30, \n",
    "                  up_down, sector]\n",
    "            window_size: Number of days in a week (5)\n",
    "            stride: Stride for sliding window\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Create weekly windows\n",
    "        self.windows = []\n",
    "        for i in range(0, len(data) - window_size + 1, stride):\n",
    "            self.windows.append(data.iloc[i:i+window_size])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        features = window[['Open', 'Close', 'High', 'Low', 'Return_Ratio',\n",
    "                        'Percent_Change_Open', 'Percentage_Change_High', 'Percentage_Change_Low',\n",
    "                        'MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30','Sector_ecnoded']].values\n",
    "        movement = window['up_down'].values[-1]  # Last day's movement\n",
    "        return_ratio = window['Return_Ratio'].values[-1]  # Last day's return ratio\n",
    "        \n",
    "        return {\n",
    "            'features': torch.FloatTensor(features),\n",
    "            'movement': torch.FloatTensor([movement]),\n",
    "            'return_ratio': torch.FloatTensor([return_ratio])\n",
    "        }\n",
    "\n",
    "class AttentiveGRU(nn.Module):\n",
    "    def __init__(self, input_dim: int = 15, hidden_dim: int = 16):\n",
    "        super(AttentiveGRU, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        output, _ = self.gru1(x)  # output shape: (batch_size, seq_len, hidden_dim)\n",
    "        output, _ = self.gru2(output)\n",
    "        # Compute attention weights\n",
    "        attention_weights = self.attention(output)  # shape: (batch_size, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Apply attention\n",
    "        a_i = torch.sum(output * attention_weights, dim=1)  # shape: (batch_size, hidden_dim)\n",
    "        return a_i\n",
    "\n",
    "class IntraSectorGAT(nn.Module):\n",
    "    def __init__(self, input_dim: int = 16, output_dim: int = 16):\n",
    "        super(IntraSectorGAT, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*output_dim, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        # x shape: (num_nodes, input_dim)\n",
    "        # adj shape: (num_nodes, num_nodes)\n",
    "        \n",
    "        Wx = self.W(x)  # shape: (num_nodes, output_dim)\n",
    "        \n",
    "        # Compute attention coefficients\n",
    "        a_input = torch.cat([Wx.repeat(1, Wx.size(0)).view(Wx.size(0) * Wx.size(0), -1),\n",
    "                            Wx.repeat(Wx.size(0), 1)], dim=1)\n",
    "        a_input = a_input.view(Wx.size(0), Wx.size(0), 2 * Wx.size(1))\n",
    "        \n",
    "        e = torch.matmul(a_input, self.a).squeeze(2)\n",
    "        attention = F.softmax(F.leaky_relu(e) * adj, dim=1)\n",
    "        \n",
    "        # Apply attention to neighbors\n",
    "        h = torch.matmul(attention, Wx)\n",
    "        return h\n",
    "\n",
    "class FinGAT(nn.Module):\n",
    "    def __init__(self, input_dim: int = 15, hidden_dim: int = 16):\n",
    "        super(FinGAT, self).__init__()\n",
    "        self.attentive_gru = AttentiveGRU(input_dim, hidden_dim)\n",
    "        self.intra_sector_gat = IntraSectorGAT(hidden_dim, hidden_dim)\n",
    "        self.inter_sector_gat = IntraSectorGAT(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.movement_head = nn.Linear(hidden_dim * 3, 1)  # Binary classification\n",
    "        self.return_head = nn.Linear(hidden_dim * 3, 1)  # Return ratio prediction\n",
    "        \n",
    "    def forward(self, x, intra_adj, inter_adj):\n",
    "        # Short-term sequential learning\n",
    "        gru_embed = self.attentive_gru(x)\n",
    "        \n",
    "        # Intra-sector modeling\n",
    "        intra_embed = self.intra_sector_gat(gru_embed, intra_adj)\n",
    "        \n",
    "        # Inter-sector modeling\n",
    "        inter_embed = self.inter_sector_gat(intra_embed, inter_adj)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([gru_embed, intra_embed, inter_embed], dim=1)\n",
    "        \n",
    "        # Multi-task predictions\n",
    "        movement_pred = torch.sigmoid(self.movement_head(combined))\n",
    "        return_pred = self.return_head(combined)\n",
    "        \n",
    "        return movement_pred, return_pred\n",
    "\n",
    "class PairwiseRankingLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 0.1):\n",
    "        super(PairwiseRankingLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # Compute pairwise differences\n",
    "        diff_pred = predictions.unsqueeze(0) - predictions.unsqueeze(1)\n",
    "        diff_target = targets.unsqueeze(0) - targets.unsqueeze(1)\n",
    "        \n",
    "        # Convert target differences to binary labels\n",
    "        target_labels = (diff_target > 0).float()\n",
    "        \n",
    "        # Compute ranking loss\n",
    "        loss = F.relu(self.margin - diff_pred * target_labels)\n",
    "        return loss.mean()\n",
    "\n",
    "def train_fingat(model: FinGAT, train_loader: DataLoader, val_loader: DataLoader, learning_rate: float = 0.001, delta: float = 0.01, epochs: int = 50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    ranking_loss = PairwiseRankingLoss()\n",
    "    movement_loss = nn.BCELoss()\n",
    "    \n",
    "    best_mrr = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            movement_pred, return_pred = model(batch['features'],\n",
    "                                             batch['intra_adj'],\n",
    "                                             batch['inter_adj'])\n",
    "            \n",
    "            # Compute losses\n",
    "            rank_loss = ranking_loss(return_pred, batch['return_ratio'])\n",
    "            mov_loss = movement_loss(movement_pred, batch['movement'])\n",
    "            \n",
    "            # Combined loss with balancing parameter\n",
    "            loss = rank_loss + delta * mov_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_mrr = evaluate_mrr(model, val_loader, k=10)\n",
    "        \n",
    "        if val_mrr > best_mrr:\n",
    "            best_mrr = val_mrr\n",
    "            torch.save(model.state_dict(), 'best_fingat_model.pt')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation MRR@10: {val_mrr:.4f}')\n",
    "\n",
    "def evaluate_mrr(model: FinGAT, data_loader: DataLoader, k: int = 10):\n",
    "    model.eval()\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            _, return_pred = model(batch['features'],\n",
    "                                 batch['intra_adj'],\n",
    "                                 batch['inter_adj'])\n",
    "            \n",
    "            # Get rankings\n",
    "            pred_ranks = torch.argsort(return_pred, descending=True)\n",
    "            true_ranks = torch.argsort(batch['return_ratio'], descending=True)\n",
    "            \n",
    "            # Compute MRR@k\n",
    "            for i in range(min(k, len(pred_ranks))):\n",
    "                if pred_ranks[i] in true_ranks[:k]:\n",
    "                    reciprocal_ranks.append(1.0 / (i + 1))\n",
    "                    break\n",
    "    \n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "# Hyperparameter configurations\n",
    "hyperparameters = {\n",
    "    'window_sizes': [5, 10, 15, 20],  # Corresponding to 1,2,3,4 weeks\n",
    "    'hidden_dims': [8, 16, 32, 64],\n",
    "    'deltas': [0, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'learning_rates': [0.0005, 0.001, 0.005],\n",
    "    'batch_sizes': [32, 64, 128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def create_sector_mappings(df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[int, List[str]]]:\n",
    "    \"\"\"\n",
    "    Create mappings between sectors and stocks\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Symbol' and 'Sector' columns\n",
    "        \n",
    "    Returns:\n",
    "        sector_to_id: Dictionary mapping sector names to sector IDs\n",
    "        sector_stocks: Dictionary mapping sector IDs to list of stock symbols in that sector\n",
    "    \"\"\"\n",
    "    # Create sector to ID mapping\n",
    "    unique_sectors = df['Sector'].unique()\n",
    "    sector_to_id = {sector: idx for idx, sector in enumerate(unique_sectors)}\n",
    "    \n",
    "    # Create sector to stocks mapping\n",
    "    sector_stocks = {}\n",
    "    for sector_id in range(len(unique_sectors)):\n",
    "        sector_name = unique_sectors[sector_id]\n",
    "        stocks_in_sector = df[df['Sector'] == sector_name]['Symbol'].tolist()\n",
    "        sector_stocks[sector_id] = stocks_in_sector\n",
    "        \n",
    "    return sector_to_id, sector_stocks\n",
    "\n",
    "def create_stock_to_sector_mapping(df: pd.DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create mapping from stock symbols to their sector IDs\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Symbol' and 'Sector' columns\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping stock symbols to their sector IDs\n",
    "    \"\"\"\n",
    "    sector_to_id, _ = create_sector_mappings(df)\n",
    "    return {row['Symbol']: sector_to_id[row['Sector']] \n",
    "            for _, row in df.iterrows()}\n",
    "\n",
    "def create_intra_sector_adjacency(df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create intra-sector adjacency matrix where stocks in the same sector are connected\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Symbol' and 'Sector' columns\n",
    "        \n",
    "    Returns:\n",
    "        Adjacency matrix as torch tensor\n",
    "    \"\"\"\n",
    "    num_stocks = len(df)\n",
    "    adjacency = torch.zeros((num_stocks, num_stocks))\n",
    "    \n",
    "    # Get stock to sector mapping\n",
    "    stock_to_sector = create_stock_to_sector_mapping(df)\n",
    "    stock_to_idx = {symbol: idx for idx, symbol in enumerate(df['Symbol'])}\n",
    "    \n",
    "    # Connect stocks in the same sector\n",
    "    for i, stock1 in enumerate(df['Symbol']):\n",
    "        for j, stock2 in enumerate(df['Symbol']):\n",
    "            if i != j and stock_to_sector[stock1] == stock_to_sector[stock2]:\n",
    "                adjacency[i, j] = 1.0\n",
    "                \n",
    "    return adjacency\n",
    "\n",
    "def create_inter_sector_adjacency(df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create inter-sector adjacency matrix where sectors are nodes\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Symbol' and 'Sector' columns\n",
    "        \n",
    "    Returns:\n",
    "        Sector-level adjacency matrix as torch tensor\n",
    "    \"\"\"\n",
    "    sector_to_id, _ = create_sector_mappings(df)\n",
    "    num_sectors = len(sector_to_id)\n",
    "    \n",
    "    # Create fully connected sector graph (excluding self-loops)\n",
    "    adjacency = torch.ones((num_sectors, num_sectors)) - torch.eye(num_sectors)\n",
    "    \n",
    "    return adjacency\n",
    "\n",
    "def create_sector_to_stock_adjacency(df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create adjacency matrix mapping sectors to their stocks\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Symbol' and 'Sector' columns\n",
    "        \n",
    "    Returns:\n",
    "        Bipartite adjacency matrix as torch tensor\n",
    "    \"\"\"\n",
    "    sector_to_id, _ = create_sector_mappings(df)\n",
    "    num_sectors = len(sector_to_id)\n",
    "    num_stocks = len(df)\n",
    "    \n",
    "    adjacency = torch.zeros((num_sectors, num_stocks))\n",
    "    stock_to_sector = create_stock_to_sector_mapping(df)\n",
    "    \n",
    "    for stock_idx, symbol in enumerate(df['Symbol']):\n",
    "        sector_id = stock_to_sector[symbol]\n",
    "        adjacency[sector_id, stock_idx] = 1.0\n",
    "        \n",
    "    return adjacency\n",
    "\n",
    "def normalize_adjacency(adjacency: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize adjacency matrix using symmetric normalization\n",
    "    \n",
    "    Args:\n",
    "        adjacency: Input adjacency matrix\n",
    "        \n",
    "    Returns:\n",
    "        Normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    # Add self-loops\n",
    "    adjacency = adjacency + torch.eye(adjacency.shape[0])\n",
    "    \n",
    "    # Calculate degree matrix\n",
    "    degree = torch.sum(adjacency, dim=1)\n",
    "    degree_sqrt_inv = torch.diag(torch.pow(degree, -0.5))\n",
    "    \n",
    "    # Symmetric normalization\n",
    "    normalized = torch.mm(torch.mm(degree_sqrt_inv, adjacency), degree_sqrt_inv)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Example usage:\n",
    "def prepare_adjacency_matrices(stock_data: pd.DataFrame) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare all necessary adjacency matrices for FinGAT\n",
    "    \n",
    "    Args:\n",
    "        stock_data: DataFrame with stock information including symbols and sectors\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing all adjacency matrices\n",
    "    \"\"\"\n",
    "    # Create basic adjacency matrices\n",
    "    intra_sector_adj = create_intra_sector_adjacency(stock_data)\n",
    "    inter_sector_adj = create_inter_sector_adjacency(stock_data)\n",
    "    sector_to_stock_adj = create_sector_to_stock_adjacency(stock_data)\n",
    "    \n",
    "    # Normalize adjacency matrices\n",
    "    normalized_intra_adj = normalize_adjacency(intra_sector_adj)\n",
    "    normalized_inter_adj = normalize_adjacency(inter_sector_adj)\n",
    "    \n",
    "    return {\n",
    "        'intra_sector': normalized_intra_adj,\n",
    "        'inter_sector': normalized_inter_adj,\n",
    "        'sector_to_stock': sector_to_stock_adj,\n",
    "        'raw_intra_sector': intra_sector_adj,\n",
    "        'raw_inter_sector': inter_sector_adj\n",
    "    }\n",
    "\n",
    "# # Example of how to use:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Sample data\n",
    "#     sample_data = pd.DataFrame({\n",
    "#         'Symbol': ['AAPL', 'MSFT', 'JPM', 'BAC', 'GOOGL'],\n",
    "#         'Sector': ['Technology', 'Technology', 'Finance', 'Finance', 'Technology']\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_sectors={\n",
    "    'Symbols' :[],\n",
    "    'Sector' : []\n",
    "}\n",
    "file_path = r'./Data_is_here'\n",
    "for file in os.listdir(file_path):\n",
    "    df = pd.read_csv(file_path+\"/\"+file)\n",
    "    # print(df.info)\n",
    "    sector = df['Sector'][5]\n",
    "    data_sectors['Symbols'].append(file[:-9])\n",
    "    data_sectors['Sector'].append(sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with structure:\n",
      "FinGAT(\n",
      "  (attentive_gru): AttentiveGRU(\n",
      "    (gru): GRU(15, 16, batch_first=True)\n",
      "    (attention): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (intra_sector_gat): IntraSectorGAT(\n",
      "    (W): Linear(in_features=16, out_features=16, bias=False)\n",
      "  )\n",
      "  (inter_sector_gat): IntraSectorGAT(\n",
      "    (W): Linear(in_features=16, out_features=16, bias=False)\n",
      "  )\n",
      "  (movement_head): Linear(in_features=48, out_features=1, bias=True)\n",
      "  (return_head): Linear(in_features=48, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Test forward pass successful!\n",
      "Movement predictions shape: torch.Size([32, 1])\n",
      "Return predictions shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def initialize_fingat(config: dict = None):\n",
    "    \"\"\"\n",
    "    Initialize FinGAT model with configuration\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing model hyperparameters\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'input_dim': 15,          # Number of features\n",
    "            'hidden_dim': 16,         # Hidden dimension size\n",
    "            'num_weeks': 1,           # Number of weeks (1,2,3,4)\n",
    "            'window_size': 5,         # Days per week\n",
    "            'batch_size': 64,\n",
    "            'learning_rate': 0.001,\n",
    "            'delta': 0.01,            # Balance parameter for multi-task learning\n",
    "        }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FinGAT(\n",
    "        input_dim=config['input_dim'],\n",
    "        hidden_dim=config['hidden_dim']\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Initialize loss functions\n",
    "    ranking_loss = PairwiseRankingLoss()\n",
    "    movement_loss = torch.nn.BCELoss()\n",
    "    \n",
    "    return model, optimizer, ranking_loss, movement_loss\n",
    "\n",
    "def create_dataloaders(stock_data: Dict[str, pd.DataFrame],\n",
    "                      adj_matrices: Dict[str, torch.Tensor],\n",
    "                      config: dict) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders\n",
    "    \"\"\"\n",
    "    # Calculate split indices\n",
    "    total_days = len(next(iter(stock_data.values())))\n",
    "    train_days = 400\n",
    "    val_days = 160\n",
    "    \n",
    "    # Split data\n",
    "    train_data = {symbol: df.iloc[:train_days] for symbol, df in stock_data.items()}\n",
    "    val_data = {symbol: df.iloc[train_days:train_days+val_days] for symbol, df in stock_data.items()}\n",
    "    test_data = {symbol: df.iloc[train_days+val_days:] for symbol, df in stock_data.items()}\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = StockDataset(train_data, adj_matrices, window_size=config['window_size'])\n",
    "    val_dataset = StockDataset(val_data, adj_matrices, window_size=config['window_size'])\n",
    "    test_dataset = StockDataset(test_data, adj_matrices, window_size=config['window_size'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def load_stock_data():\n",
    "    \n",
    "    file_path = './Preprocessed_data'\n",
    "    stock_data = {}\n",
    "    for file in os.listdir(file_path):\n",
    "        symbol = file.split('.')[0]\n",
    "        df = pd.read_csv(os.path.join(file_path, file))\n",
    "        stock_data[symbol] = df\n",
    "    return stock_data\n",
    "\n",
    "# Example usage\n",
    "def setup_and_train():\n",
    "    \"\"\"Complete setup and training pipeline\"\"\"\n",
    "    \n",
    "    # 1. Define configuration\n",
    "    config = {\n",
    "        'input_dim': 15,\n",
    "        'hidden_dim': 16,\n",
    "        'num_weeks': 1,\n",
    "        'window_size': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'delta': 0.01,\n",
    "        'epochs': 50\n",
    "    }\n",
    "    \n",
    "    # 2. Load and prepare data\n",
    "    stock_data = load_stock_data()\n",
    "    # stock_data = \n",
    "    features_dict = data_sectors\n",
    "    adj_matrices = prepare_adjacency_matrices(stock_data)\n",
    "    \n",
    "    # 3. Initialize model and components\n",
    "    model, optimizer, ranking_loss, movement_loss = initialize_fingat(config)\n",
    "    \n",
    "    # 4. Create dataloaders\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        features_dict, adj_matrices, config\n",
    "    )\n",
    "    \n",
    "    # 5. Training loop\n",
    "    best_val_mrr = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            movement_pred, return_pred = model(\n",
    "                batch['features'],\n",
    "                batch['intra_adj'],\n",
    "                batch['inter_adj']\n",
    "            )\n",
    "            \n",
    "            # Calculate losses\n",
    "            rank_loss = ranking_loss(return_pred, batch['return_ratio'])\n",
    "            mov_loss = movement_loss(movement_pred, batch['movement'])\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = rank_loss + config['delta'] * mov_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        val_mrr = evaluate_mrr(model, val_loader)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_mrr > best_val_mrr:\n",
    "            best_val_mrr = val_mrr\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_mrr': val_mrr,\n",
    "                'config': config\n",
    "            }, 'best_fingat_model.pt')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{config[\"epochs\"]}:')\n",
    "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation MRR: {val_mrr:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Quick initialization for testing\n",
    "def quick_test():\n",
    "    \"\"\"Initialize model with default settings for testing\"\"\"\n",
    "    config = {\n",
    "        'input_dim': 15,\n",
    "        'hidden_dim': 16,\n",
    "        'num_weeks': 1,\n",
    "        'window_size': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'delta': 0.01\n",
    "    }\n",
    "    \n",
    "    model, optimizer, ranking_loss, movement_loss = initialize_fingat(config)\n",
    "    print(\"Model initialized with structure:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Test with random data\n",
    "    batch_size = 32\n",
    "    window_size = 5\n",
    "    num_features = 15\n",
    "    test_features = torch.randn(batch_size, window_size, num_features)\n",
    "    test_intra_adj = torch.ones(batch_size, batch_size)\n",
    "    test_inter_adj = torch.ones(32, 32)  # Assuming 10 sectors\n",
    "    \n",
    "    movement_pred, return_pred = model(test_features, test_intra_adj, test_inter_adj)\n",
    "    print(\"\\nTest forward pass successful!\")\n",
    "    print(f\"Movement predictions shape: {movement_pred.shape}\")\n",
    "    print(f\"Return predictions shape: {return_pred.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quick_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
